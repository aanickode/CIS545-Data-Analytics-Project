# -*- coding: utf-8 -*-
"""CIS 545 Final submission

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hn0zoViEJhqEf-c0lTU5ggOPAJgLqBe6

Import statements and loading geopandas package
"""

import pandas as pd
import scipy
import numpy as np
import time
import json
import math
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt 
import io      
import bokeh.io
from sklearn.impute import KNNImputer 
from bokeh.io import output_notebook, show, output_file
from bokeh.plotting import figure
from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar, Slider, HoverTool
from bokeh.palettes import brewer
from bokeh.io import curdoc, output_notebook
from bokeh.layouts import widgetbox, row, column
from bokeh.resources import INLINE
import seaborn as sn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import mean_squared_error
from math import sqrt
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
from sklearn import linear_model
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import datetime
from pylab import rcParams

!pip install --upgrade geopandas
!pip install --upgrade pyshp

!pip install --upgrade shapely

!pip install --upgrade descartes
import geopandas as gpd

"""### Part 1: Data Preparation/Cleanup

As part of data cleanup, we go through data we hand selected from world bank. World bank has public data on each category of development including Poverty, Health, Gender, the Public Sector, the Environment, Education, National Accounts (information on GDP etc), Transportation, Debt, Financial Services, Social Health, and Social Protection and Labor for each year from 1985 to 2019. We will first add data for years 1990 to 2015 and then add remaining data. The other years have very limited data. Out of these categories our goal was to find 12 particular indicators out of these categories that can help us sucessfully predict which year a country is developed (1) or not (0) based on these indicators.

To find these indicators we go through the following steps:
1.   We go through each category of development from the World bank website.
2.   We only choose indicators in each category that has data for more than 180 countries (there are 193 recognized countries total excluding sovereign territories), since we will use KKN Imputer to fill in values that are missing. We ideally do this so we are not imputing data for too many indicator values as that will give us a drastically low accurancy while predicting developed or not.
3. We set threshold (thresh = 23) for number of years of data as 23. Out of 25 years, we need data for at least 23 years for the same reasons as above. 
4. After finding all indicators in each category we find the best indicator out for each category dependent on which are the best dvelopmental factors. (We use a reserach study conducted by NPO World Outreach to manually select the best indicators for development absed on their rubric out of the data we have)
"""

na_df = pd.read_csv("NationalAccounts.csv")
#replace .. with null values
na_df = na_df.replace('..',np.NaN)
# need data for at least 23 years, else drop the entire indicator row
na_df = na_df.dropna(axis=0, thresh = 23)
# we do not need the series and country code columns
na_df = na_df.drop(columns=['Series Code', 'Country Code'])
# only need year values (related cleanup)
na_df.columns = na_df.columns.str[0:4]
na_df = na_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
# we print number of countries and number of indicators we have in the dataframe
# after the conditions we placed are established/cleaned
print('Number of countries in dataframe:', na_df['Country'].nunique())
print('Number of indicators in dataframe', na_df['Indicator Name'].nunique())
na_df.reset_index()
# we print out how many countries have data for each indicator for each year
na_df = na_df.groupby(by = 'Indicator Name').count()
# by teh metrics we set above we only want data if tehre is at least 180 countries
#represented amongst ALL years of data
na_df = na_df[na_df['Country'] >= 180]
#we print out the dataframe
na_df

"""Out of curiosity we want to see if the countries we are missing data for are different amonst two different indicators in this catagory. We take two random indicators and find which ountries are missing and compare them. We noticed that the countries missing data are very different amongst indicators we cannot just omit data for a few countries that have no data without having a significant biases/ skewed results. 

For example for the education expenditure indicator we were msising data for the UAE, Bosnia/Herzegovinia, DPRK, Tuvalu, Nauro, Palau, and Montegnero and for Carbon dioxide damage we were missing data for South sudan, Monaco, Serbia, Leichtenstein, Montenegro, San Marino, and Timor Leste, The only overlapping country presnet in both these sets of countries is Montenegro where no data was found for either of these two indicators. Based on this we can reasonably presume, that the countries with missing data are different for each indicator. 

"""

na_df = pd.read_csv("NationalAccounts.csv")
na_df = na_df.replace('..',np.NaN)
na_df = na_df.dropna(axis=0, thresh = 23)
na_df = na_df.drop(columns=['Series Code', 'Country Code'])
na_df.columns = na_df.columns.str[0:4]
na_df = na_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
countries = na_df['Country'].unique().tolist()
na_df = na_df[na_df['Indicator Name'] == 'Adjusted savings: education expenditure (% of GNI)']
na_df = na_df['Country'].tolist()
# we subtract the set of countries missing from the set of all countries and find teh countries that 
# are missing for this particular indicator
countries_missing_education = list(set(countries) - set(na_df))
countries_missing_education

na_df = pd.read_csv("NationalAccounts.csv")
na_df = na_df.replace('..',np.NaN)
na_df = na_df.dropna(axis=0, thresh = 23)
na_df = na_df.drop(columns=['Series Code', 'Country Code'])
na_df.columns = na_df.columns.str[0:4]
na_df = na_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
countries = na_df['Country'].unique().tolist()
na_df = na_df[na_df['Indicator Name'] == 'Adjusted savings: carbon dioxide damage (current US$)']
na_df = na_df['Country'].tolist()
countries_missing_CO2 = list(set(countries) - set(na_df))
countries_missing_education

"""We replicate this analysis with the parameters we described for the National Accounts category for every category group and analyze the indicators till we find 12 indicators that accuartely described development according to our development research article. We will print out the dataframe containing indicators displayed for each of these cateories with the values of datapoints representing how many datapoints (countries) are available for each indicator for each year. Some of the dataframes contain no data as they did not meet our treshold requirements we set in the instructions above. We strictly adhere to these requirements since without enough data our model will not be a great predictor of development. This was because the world bank did not collect enough data as mechanisms were not present during most years as they explained on their website! Note: we only expect 180 countries to be represnetd for each indicator across all years, so a few years may have less than or greater than 180 datapoints. We are only interested the aggregate amount of datapoints across all years."""

#in this dataframe we can see that before thed drop based of 180 countries we had 93 indicators present.
# However, even though it states 188 countries were present befroe drop, each indicator had less than 180 datapoints
# so, none of the indicatros could be used for our analysis unfortunately
debt_df = pd.read_csv("Debt.csv")
debt_df = debt_df.replace('..',np.NaN)
debt_df = debt_df.dropna(axis=0, thresh = 23)
debt_df = debt_df.drop(columns=['Series Code', 'Country Code'])
debt_df.columns = debt_df.columns.str[0:4]
debt_df = debt_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print('Number of countries before drop:', debt_df['Country'].nunique())
print('Number of indicators before drop', debt_df['Indicator Name'].nunique())
debt_df.reset_index()
debt_df = debt_df.groupby(by = 'Indicator Name').count()
debt_df = debt_df[debt_df['Country'] >= 180]
print('Number of countries in dataframe:', len(debt_df.columns)-1)
print('Number of indicators in dataframe', len(debt_df.index))
debt_df

education_df = pd.read_csv("Education_ds.csv")
education_df = education_df.replace('..',np.NaN)
education_df = education_df.dropna(axis=0, thresh = 22)
education_df = education_df.drop(columns=['Series Code', 'Country Code'])
education_df.columns = education_df.columns.str[0:4]
education_df = education_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(education_df['Country'].nunique())
print(education_df['Indicator Name'].nunique())
education_df.reset_index()
education_df = education_df.groupby(by = 'Indicator Name').count()
education_df = education_df[education_df['Country'] >= 180]
education_df

environment_df = pd.read_csv("Environment_ds.csv")
environment_df = environment_df.replace('..',np.NaN)
environment_df = environment_df.dropna(axis=0, thresh = 24)
environment_df = environment_df.drop(columns=['Series Code', 'Country Code'])
environment_df.columns = environment_df.columns.str[0:4]
environment_df = environment_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(environment_df['Country'].nunique())
print(environment_df['Indicator Name'].nunique())
environment_df.reset_index()
environment_df = environment_df.groupby(by = 'Indicator Name').count()
environment_df = environment_df[environment_df['Country'] >= 185]
environment_df

fs_df = pd.read_csv("FinancialSector.csv")
fs_df = fs_df.replace('..',np.NaN)
fs_df = fs_df.dropna(axis=0, thresh = 23)
fs_df = fs_df.drop(columns=['Series Code', 'Country Code'])
fs_df.columns = fs_df.columns.str[0:4]
fs_df = fs_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(fs_df['Country'].nunique())
print(fs_df['Indicator Name'].nunique())
fs_df.reset_index()
fs_df = fs_df.groupby(by = 'Indicator Name').count()
fs_df = fs_df[fs_df['Country'] >= 170]
fs_df

gender_df = pd.read_csv("Gender.csv")
gender_df = gender_df.replace('..',np.NaN)
gender_df = gender_df.dropna(axis=0, thresh = 24)
gender_df = gender_df.drop(columns=['Series Code', 'Country Code'])
gender_df.columns = gender_df.columns.str[0:4]
gender_df = gender_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(gender_df['Country'].nunique())
print(gender_df['Indicator Name'].nunique())
gender_df.reset_index()
gender_df = gender_df.groupby(by = 'Indicator Name').count()
gender_df = gender_df[gender_df['Country'] >= 185]
gender_df

health_df = pd.read_csv("Health.csv")
health_df = health_df.replace('..',np.NaN)
health_df = health_df.dropna(axis=0, thresh = 24)
health_df = health_df.drop(columns=['Series Code', 'Country Code'])
health_df.columns = health_df.columns.str[0:4]
health_df = health_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(health_df['Country'].nunique())
print(health_df['Indicator Name'].nunique())
health_df.reset_index()
health_df = health_df.groupby(by = 'Indicator Name').count()
health_df = health_df[health_df['Country'] >= 180]
#health_df = health_df.transpose()
health_df

na_df = pd.read_csv("NationalAccounts.csv")
na_df = na_df.replace('..',np.NaN)
na_df = na_df.dropna(axis=0, thresh = 24)
na_df = na_df.drop(columns=['Series Code', 'Country Code'])
na_df.columns = na_df.columns.str[0:4]
na_df = na_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(na_df['Country'].nunique())
print(na_df['Indicator Name'].nunique())
na_df.reset_index()
na_df = na_df.groupby(by = 'Indicator Name').count()
na_df = na_df[na_df['Country'] >= 185]
na_df

# here is a good example of why we also print out number of datapoints (country data) avaiable
# for each year and indicator. Even though there are 93 country data present in aggregate for each indicator we can see there 
# virtually no data present in many of the years. This is why we can't use most of the indicators since data is not available for a huge chunk of the
# years even though it is aviable for all the countreis after year 2000 etc.
poverty_df = pd.read_csv("Poverty.csv")
poverty_df = poverty_df.replace('..',np.NaN)
poverty_df = poverty_df.dropna(axis=0, thresh = 1)
poverty_df = poverty_df.drop(columns=['Series Code', 'Country Code'])
poverty_df.columns = poverty_df.columns.str[0:4]
poverty_df = poverty_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(poverty_df['Country'].nunique())
print(poverty_df['Indicator Name'].nunique())
poverty_df.reset_index()
poverty_df = poverty_df.groupby(by = 'Indicator Name').count()
poverty_df = poverty_df[poverty_df['Country'] >= 160]
poverty_df

public_data = pd.read_csv("PublicSector data.csv")
public_data = public_data.replace('..',np.NaN)
public_data = public_data.dropna(axis=0, thresh = 24)
public_data = public_data.drop(columns=['Series Code', 'Country Code'])
public_data.columns = public_data.columns.str[0:4]
public_data = public_data.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(public_data['Country'].nunique())
print(public_data['Indicator Name'].nunique())
public_data.reset_index()
public_data = public_data.groupby(by = 'Indicator Name').count()
public_data = public_data[public_data['Country'] >= 180]
public_data

sh_df = pd.read_csv("SocialHealth.csv")
sh_df = sh_df.replace('..',np.NaN)
sh_df = sh_df.dropna(axis=0, thresh = 24)
sh_df = sh_df.drop(columns=['Series Code', 'Country Code'])
sh_df.columns = sh_df.columns.str[0:4]
sh_df = sh_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(sh_df['Country'].nunique())
print(sh_df['Indicator Name'].nunique())
sh_df.reset_index()
sh_df = sh_df.groupby(by = 'Indicator Name').count()
sh_df = sh_df[sh_df['Country'] >= 180]
sh_df

spl_df = pd.read_csv("SocialProtectionAndLabor.csv")
spl_df = spl_df.replace('..',np.NaN)
spl_df = spl_df.dropna(axis=0, thresh = 24)
spl_df = spl_df.drop(columns=['Series Code', 'Country Code'])
spl_df.columns = spl_df.columns.str[0:4]
spl_df = spl_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(spl_df['Country'].nunique())
print(spl_df['Indicator Name'].nunique())
spl_df.reset_index()
spl_df = spl_df.groupby(by = 'Indicator Name').count()
spl_df = spl_df[spl_df['Country'] >= 180]
spl_df

transport_df = pd.read_csv("Transportation.csv")
transport_df = transport_df.replace('..',np.NaN)
transport_df = transport_df.dropna(axis=0, thresh = 24)
transport_df = transport_df.drop(columns=['Series Code', 'Country Code'])
transport_df.columns = transport_df.columns.str[0:4]
transport_df = transport_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
print(transport_df['Country'].nunique())
print(transport_df['Indicator Name'].nunique())
transport_df.reset_index()
transport_df = transport_df.groupby(by = 'Indicator Name').count()
transport_df = transport_df[transport_df['Country'] >= 185]
transport_df

# we reset df values after we find best indicators since we altered these dataframes while doing the analysis
debt = pd.read_csv("Debt.csv")
education = pd.read_csv("Education_ds.csv")
environment = pd.read_csv("Environment_ds.csv")
financial = pd.read_csv("FinancialSector.csv")
gender = pd.read_csv("Gender.csv")
health = pd.read_csv("Health.csv")
nationalAccounts = pd.read_csv("NationalAccounts.csv")
poverty = pd.read_csv("Poverty.csv")
publicSector = pd.read_csv("PublicSector data.csv")
socialHealth = pd.read_csv("SocialHealth.csv")
socialProtectionAndLabor = pd.read_csv("SocialProtectionAndLabor.csv")
transportation = pd.read_csv("Transportation.csv")
# we combine all dataframes into one massive dataframe
combined_df = pd.concat([debt, education, environment, financial, gender, health], axis=0)
combined_df = pd.concat([combined_df, nationalAccounts, poverty, publicSector, socialHealth, socialProtectionAndLabor, transportation], axis=0)
# change name of country code to just CODE for readability
combined_df = combined_df.rename(columns={'Country Code': 'CODE'})

# change .. (missing data) to null values
combined_df = combined_df.replace('..',np.NaN)
# cleaning up combined dataset
combined_df = combined_df.drop(columns=['Series Code'])
combined_df.columns = combined_df.columns.str[0:4]
combined_df = combined_df.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
#we extract only the rows of the indicators we want out of our massive dataframe of all 1500+ indicators based off of our analysis
combined_df = combined_df[(combined_df['Indicator Name'] == 'GDP per capita (current US$)') |
                          (combined_df['Indicator Name'] == 'Rural population (% of total population)') |
                          (combined_df['Indicator Name'] == 'Population growth (annual %)') |
                          (combined_df['Indicator Name'] == 'Mortality rate, under-5 (per 1,000 live births)') |
                          (combined_df['Indicator Name'] == 'Birth rate, crude (per 1,000 people)') |
                          (combined_df['Indicator Name'] == 'Life expectancy at birth, total (years)') |
                          (combined_df['Indicator Name'] == 'Adjusted savings: education expenditure (% of GNI)') |
                          (combined_df['Indicator Name'] == 'Immunization, DPT (% of children ages 12-23 months)') |
                          (combined_df['Indicator Name'] == 'Immunization, measles (% of children ages 12-23 months)') |
                          (combined_df['Indicator Name'] == 'Individuals using the Internet (% of population)') |
                          (combined_df['Indicator Name'] == 'Renewable energy consumption (% of total final energy consumption)') |
                          (combined_df['Indicator Name'] == 'CO2 emissions (metric tons per capita)')]

#we store indicator code and ocuntry name in lists and move them to front of dataframe to make dataframe more readable
indicator_name = combined_df['Indicator Name'].tolist()
code_name = combined_df['CODE'].tolist()
country_name = combined_df['Country'].tolist()
combined_df = combined_df.drop(columns=['Country', 'Indicator Name'])
combined_imputed = combined_df
#change column names to years
combined_df = pd.DataFrame(combined_imputed, columns=['1990', '1991', '1992', '1993', '1994', '1995', '1996',
                                                      '1997', '1998', '1999', '2000', '2001', '2002', '2003',
                                                      '2004', '2005', '2006', '2007', '2008', '2009', '2010',
                                                      '2011', '2012', '2013', '2014', '2015'])
combined_df.insert(0, "Indicator Name", indicator_name, True)
combined_df.insert(0, "Country", country_name, True)
combined_df.insert(0, "CODE", code_name, True)
combined2_df = combined_df
combined2_df

#collect values for each of these indicators from years 1985-1990 and 2015-2019 in dataframe
#called extra_data and add them to our dataframe
extra_data = pd.read_csv("extra_data.csv")
extra_data = extra_data.replace('..',np.NaN)
extra_data = extra_data.drop(columns=['Series Code'])
extra_data = extra_data.rename(columns={'Country Code': 'CODE'})
extra_data.columns = extra_data.columns.str[0:4]
extra_data = extra_data.rename(columns={'Coun': 'Country', 'Seri': 'Indicator Name'})
extra_data.dropna(how ='all')
#drop irrelevant rows
extra_data = extra_data[extra_data['Indicator Name'] != 'Data from database: World Development Indicators']
extra_data = extra_data[extra_data['Indicator Name'] != 'Last Updated: 11/23/2021']
extra_data.drop(extra_data.tail(5).index,inplace = True)
extra_data.dropna(how ='all')
extra_data = extra_data[extra_data['Country'] != 'NaN']
combined_df = extra_data
#want to find missing country rows within both combined_df (combined2_df is the old datafraem without extra data)
#combined_df is new dataframe with the extra_data df concatenated with the old combined_df 
x = combined2_df['Country'].unique().tolist()
y = combined_df['Country'].unique().tolist()
missing = list(set(x) - set(y))
#want to add country codes for the geoplot in EDA since will be useing same dataframe
result = pd.merge(combined2_df, combined_df, how="left",on=["Indicator Name","Country", 'CODE'])
result = pd.melt(result, id_vars=['Country', 'CODE'], value_vars=['1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019'])
result = result.groupby(by=['Country', 'CODE', 'variable'])['value'].apply(list)
result = pd.DataFrame(result)
result['CO2 emissions (metric tons per capita)'], result['Renewable energy consumption (% of total final energy consumption)'], result['Rural population (% of total population)'], result['Birth rate, crude (per 1,000 people)'], result['Immunization, DPT (% of children ages 12-23 months)'], result['Immunization, measles (% of children ages 12-23 months)'], result['Life expectancy at birth, total (years)'], result['Mortality rate, under-5 (per 1,000 live births)'], result['Population growth (annual %)'], result['Adjusted savings: education expenditure (% of GNI)'], result['GDP per capita (current US$)'], result['Individuals using the Internet (% of population)'] = zip(*result.pop('value'))

#we will now create a column with data on whterh a country is developed or not. 
#This data is according to HDI indicators telling us since when a country was considered a devloping country
#so far no country at least on teh abssi of HDI was developed and is not back to teh developing stage
#so once a country becomes developed it normally stays dveloped so when its index geos from 0 to 1, it stays 1
#each country is populated with 0s for each year and then we replace the country and its year row with 1s after the year each country got developed
result['developed or not'] = 0
result = result.reset_index()
result = result.rename(columns = {'variable' : 'Years'})
result.loc[(result['Country'] == 'Lithuania') & (result['Years'] >= '2005'), 'developed or not'] = 1
#for x in (result[(result['Country'] == 'Lithuania') & (result['Years'] >= '2005')]):
 # result['developed or not'] = 1
# have to manually enter data for each country
#all other countries are assumed to be undeveloped so are populated with 0s.
x = result[(result['Country'] == 'Lithuania') & (result['Years'] >= '2002')]
result.loc[(result['Country'] == 'Latvia') & (result['Years'] >= '2005'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Estonia') & (result['Years'] >= '2003'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Israel') & (result['Years'] >= '1991'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Slovenia') & (result['Years'] >= '1998'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Czech Republic') & (result['Years'] >= '2001'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Slovak Republic') & (result['Years'] >= '2006'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Portugal') & (result['Years'] >= '2005'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Korea, Rep.') & (result['Years'] >= '1999'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Greece') & (result['Years'] >= '2001'), 'developed or not'] = 1
result.loc[(result['Country'] == 'New Zealand') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Spain') & (result['Years'] >= '1995'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Finland') & (result['Years'] >= '1994'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Ireland') & (result['Years'] >= '1996'), 'developed or not'] = 1
result.loc[(result['Country'] == 'United Kingdom') & (result['Years'] >= '1992'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Iceland') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Italy') & (result['Years'] >= '1995'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Sweden') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Australia') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Belgium') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Canada') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'France') & (result['Years'] >= '1993'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Austria') & (result['Years'] >= '1992'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Germany') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Japan') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Luxembourg') & (result['Years'] >= '1992'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Denmark') & (result['Years'] >= '1991'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Netherlands') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'United States') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Norway') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Switzerland') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Andorra') & (result['Years'] >= '1990'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Chile') & (result['Years'] >= '2007'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Hungary') & (result['Years'] >= '2005'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Poland') & (result['Years'] >= '2003'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Malta') & (result['Years'] >= '2003'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Cyprus') & (result['Years'] >= '2001'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Singapore') & (result['Years'] >= '1999'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Croatia') & (result['Years'] >= '2007'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Uruguay') & (result['Years'] >= '2014'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Bahamas, The') & (result['Years'] >= '2016'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Turkey') & (result['Years'] >= '2015'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Kuwait') & (result['Years'] >= '2014'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Oman') & (result['Years'] >= '2012'), 'developed or not'] = 1
result.loc[(result['Country'] == 'San Marino') & (result['Years'] >= '2012'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Bahrain') & (result['Years'] >= '2012'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Saudi Arabia') & (result['Years'] >= '2010'), 'developed or not'] = 1
result.loc[(result['Country'] == 'United Arab Emirates') & (result['Years'] >= '2004'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Brunei Darussalam') & (result['Years'] >= '1999'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Qatar') & (result['Years'] >= '1996'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Costa Rica') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Panama') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Bulgaria') & (result['Years'] >= '2015'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Palau') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Romania') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Malaysia') & (result['Years'] >= '2016'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Kazakhstan') & (result['Years'] >= '2015'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Russian Federation') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Barbados') & (result['Years'] >= '2011'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Seychelles') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'St. Kitts and Nevis') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Trinidad and Tobago') & (result['Years'] >= '2005'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Liechtenstein') & (result['Years'] >= '2000'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Georgia') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Mauritius') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Serbia') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Nauru') & (result['Years'] >= '2019'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Montenegro') & (result['Years'] >= '2013'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Antigua and Barbuda') & (result['Years'] >= '2012'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Belarus') & (result['Years'] >= '2012'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Argentina') & (result['Years'] >= '2006'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Mexico') & (result['Years'] >= '1994'), 'developed or not'] = 1
result.loc[(result['Country'] == 'Monaco') & (result['Years'] >= '1994'), 'developed or not'] = 1
x = result[(result['Country'] == 'United States') & (result['Years'] >= '1980')]
# there are 5046 null values in this dataframe which we must compute using KNN Imputer as shown in class
#result.isna().sum().sum() 5046
result

country = result['Country']
result = result.drop(columns=['Country'])
years = result['Years']
result = result.drop(columns=['Years'])
code = result['CODE']
result = result.drop(columns=['CODE'])
imputer = KNNImputer(n_neighbors=2)
df_filled = imputer.fit_transform(result)
combined_df = pd.DataFrame(df_filled, columns=['CO2 emissions (metric tons per capita)', 'Renewable energy consumption (% of total final energy consumption)', 'Rural population (% of total population)', 'Birth rate, crude (per 1,000 people)', 'Immunization, DPT (% of children ages 12-23 months)', 'Immunization, measles (% of children ages 12-23 months)', 'Life expectancy at birth, total (years)', 'Mortality rate, under-5 (per 1,000 live births)', 'Population growth (annual %)', 'Adjusted savings: education expenditure (% of GNI)', 'GDP per capita (current US$)', 'Individuals using the Internet (% of population)', 'Developed?'])
combined_df.insert(0, "Years", years, True)
combined_df.insert(0, "Country", country, True)
combined_df.insert(0, "CODE", code, True)
combined_df["Country-Year"] = combined_df["Country"] + "-" + combined_df["Years"].astype(str)
country_year = combined_df['Country-Year']
combined_df = combined_df.drop(columns=['Country', 'Years', 'Country-Year'])
combined_df.insert(0, "Country-Year", country_year, True)
combined_df['Developed?'] = combined_df['Developed?'].astype(int)
x = combined_df[combined_df['Developed?'] == 1].count()
x

combined_df

"""### Part 2: Exploratory Data Analysis (EDA)

We graph the rural and immunization populations on a cloropleth world map to see how the rural population changed from 1990 to 2019. We also create a covariance matrix and a heat map to see the correlation between each of our 12 indices we will use to predict when a country will be considered developed.
"""

# here we are importing the file of all the country's coordinates in order to map them out 
# we take them from a website cited at end of project
shapefile = 'graphing_data/ne_110m_admin_0_countries.shp'
gdf = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]
gdf.columns = ['country', 'country_code', 'geometry']
gdf.head()

# similar to as done on model, we take out antarctica since, there has obviously been no
# development in Antarctica 
print(gdf[gdf['country'] == 'Antarctica'])
gdf = gdf.drop(gdf.index[159])
gdf.info()

# We merge our existing data with the data we imported so the two datasets are now connected
# Now we have a final dataframe named merged2 with all teh ocuntries coordinates and their respective information
combined_df['Country'], combined_df['Year'] = combined_df['Country-Year'].str.split('-', 1).str
df_2016 = combined_df[combined_df['Year'] == '2016']
merged = gdf.merge(df_2016, left_on = 'country_code', right_on = 'CODE', how = 'left')
merged.fillna('No data', inplace = True)
df_1990 = combined_df[combined_df['Year'] == '1990']
merged2 = gdf.merge(df_1990, left_on = 'country_code', right_on = 'CODE', how = 'left')
merged2.fillna('No data', inplace = True)
merged2

#we load all json data into merged_jason and merged2.json
#json.dumps is a json encoder/decoder
merged_json = json.loads(merged.to_json())
merged2_json = json.loads(merged.to_json())
json_data = json.dumps(merged_json)
json2_data = json.dumps(merged_json)

# we cleanup this dataframe so we can prepare to create our cloropleth map
combined_df['Year'] = combined_df['Year'].str[0:4]
combined_df = combined_df[combined_df['Year'] != 'Biss']
combined_df = combined_df[combined_df['Year'] != 'Lest']
combined_df['Year'] = combined_df['Year'].astype(int)
combined_df['Year'].unique()
combined_df['CODE'].nunique()

combined_df = combined_df.rename(columns={'Rural population (% of total population)': 'rural', 'Immunization, DPT (% of children ages 12-23 months)' : 'immunization'})
combined_df

#this is the code to create our cloropleth graph 
geosource = GeoJSONDataSource(geojson = json2_data)
#define colors
color_p = brewer['YlGnBu'][8]
#dark should be highest immunized
color_p = color_p[::-1]
hover = HoverTool(tooltips = [ ('Country Name','@country'),('% immunized', '@immunization')])
color_mapper = LinearColorMapper(palette = color_p, low = 0, high = 40)
tick_labels = {'0': '0%', '50': '50%', '60':'60%', '70':'70%', '80':'80%', '90':'90%', '98': '>98%'}
color_labels = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20,
border_line_color = None,location = (0,0), orientation = 'horizontal', major_label_overrides = tick_labels)
p = figure(title = 'Immunization, DPT (% of children ages 12-23 months)', plot_height = 600 , plot_width = 1000, tools = [hover])
p.patches('xs','ys', source = geosource,fill_color = {'field' :'Immunization, DPT (% of children ages 12-23 months)', 'transform' : color_mapper},
          line_color = 'black', line_width = 0.25, fill_alpha = 1)
p.add_layout(color_labels, 'below')
show(p)

# just like done in our source linked below, we create a slider
# that uses each json value to cretae graph for each year as done above 
# Our hover tool alows us to hover over each country to see what value it has
def json_data(selectedYear):
    yr = selectedYear
    df_yr = combined_df[combined_df['Year'] == yr]
    merged = gdf.merge(df_yr, left_on = 'country_code', right_on ='CODE', how = 'left')
    merged.fillna('No data', inplace = True)
    merged_json = json.loads(merged.to_json())
    json_data = json.dumps(merged_json)
    return json_data

geosource = GeoJSONDataSource(geojson = json_data(2016))
color_p = brewer['YlGnBu'][8]
color_p = color_p[::-1]
color_mapper = LinearColorMapper(palette = color_p, low = 0, high = 40, nan_color = '#d9d9d9')
tick_labels = {'0': '0%', '5': '5%', '10':'10%', '15':'15%', '20':'20%', '25':'25%',
               '30':'30%','35':'35%', '40': '>40%'}

hover = HoverTool(tooltips = [ ('Country/region','@country'),('% rural', '@rural')])
color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 500, height = 20,
border_line_color=None,location = (0,0), orientation = 'horizontal', major_label_overrides = tick_labels)
p = figure(title = 'Rural population (% of total population)', plot_height = 600 , plot_width = 1000,
           toolbar_location = None, tools = [hover])
p.patches('xs','ys', source = geosource,fill_color = {'field' :'rural', 'transform' : color_mapper},
          line_color = 'black', line_width = 0.25, fill_alpha = 1)
p.add_layout(color_bar, 'below')

def update_plot(attr, old, new):
    yr = slider.value
    new_data = json_data(yr)
    cr = select.value
    input_field = format_df.loc[format_df['verbage'] == cr, 'field'].iloc[0]
    p = make_plot(input_field)    
    layout = column(p, widgetbox(select), widgetbox(slider))
    curdoc().clear()
    curdoc().add_root(layout) 
    geosource.geojson = new_data

slider = Slider(title = 'Year',start = 1975, end = 2016, step = 1, value = 2016)
slider.on_change('value', update_plot)
layout = column(p,widgetbox(slider))
curdoc().add_root(layout)
output_notebook()
show(layout)

"""Here we can see a clear world map of how rural populations are divided in developed and the non-developed world. Clearly countries that are developed have lower rural populations as you can tell (go hover above them!) versus countries that have significantly larger populations."""

full_df = combined_df
combined_df = combined_df.drop(columns=['Country-Year', 'CODE', 'Country', 'Year'])
combined_df

# we create covariance matrix like we did in class to see if any of our features
# have any colinearity which we can see with the darker numbers in the heatmap below
covariance_df = combined_df
covariance_df.columns = covariance_df.columns.str[0:16]
corrMatrix = covariance_df.corr()

sn.heatmap(corrMatrix, annot=True)
plt.show()

# we are isolating the histogram data for just rural columns so, we can graoh it for both developed and 
#un-developed countries using the histogram
hist_df = full_df
hist_df = hist_df.drop(columns = ['Country-Year', 'CODE', 'CO2 emissions (metric tons per capita)', 'Renewable energy consumption (% of total final energy consumption)', 'Birth rate, crude (per 1,000 people)', 'immunization',
                                  'Immunization, measles (% of children ages 12-23 months)', 'Life expectancy at birth, total (years)', 'Mortality rate, under-5 (per 1,000 live births)', 'Population growth (annual %)', 'Adjusted savings: education expenditure (% of GNI)', 'GDP per capita (current US$)',
                                  'Individuals using the Internet (% of population)', 'Country', 'Year'])
hist_df

# we create two histograms with the numebr of countries and years aggregated on the y-axis which 
#is labelled index no. and the percentage of a country that is rural in the y axis
dev_hist = hist_df['rural'].hist(by=hist_df['Developed?'], sharex=True, sharey=True, layout = (2, 1))
for ax in dev_hist.flatten():
    ax.set_xlabel("% rural")
    ax.set_ylabel("index no.")

"""As we see in this plot, the 0 histogram representing undeveloped countries has much higher percentage of rural poulation between 40 to 80 percent, compared to a relative tiny population of rural population in the developed histogram, apart from the small minority with rural population of around 80%, which may represent a smaller country who is developed but its inhabitants are categorized as "rural.""""

# this is just for fun - extrapolation
box_whisker_df = full_df
box_whisker_df = box_whisker_df[box_whisker_df['Country'] == 'Iceland']
box_whisker_df = box_whisker_df[(box_whisker_df['Year'] >= 1985) & (box_whisker_df['Year'] <= 2000)]
sns.set_style("whitegrid")
sns.boxplot(x = 'Year', y = 'rural', data = box_whisker_df)

# this is tehe box and whisker plot for developed countries as we isolated all rows with developed values of 1 
# we clearly have a few outliers but for the most part the rural population is pretty low as we also observed in the histogram
box_whisker_df = hist_df
box_whiskerdev_df = box_whisker_df[box_whisker_df['Developed?'] == 1]
box_whiskerdev_df.reset_index()
box_whiskerdev_df['rural'].describe()
boxplot = box_whiskerdev_df.boxplot(column=['rural'])
boxplot.plot()
boxplot.set_ylabel('rural %')
boxplot.set_xlabel('countries')
plt.show()

# this is the box and whisker plot for undeveloped countries. With less outliers, we can see overall more % of people live in rural areas in developed countries
# we isolated all the rows with developed values of 0.
box_whiskerundev_df = box_whisker_df[box_whisker_df['Developed?'] == 0]
box_whiskerundev_df.reset_index()
box_whiskerundev_df['rural'].describe()
boxplot = box_whiskerundev_df.boxplot(column=['rural'])
boxplot.plot()
boxplot.set_ylabel('rural %')
boxplot.set_xlabel('countries')
plt.show()

"""### Part 3: Modeling Indicator Data to Predict Country Development

We will now train and test our development index for the countries and years using the indicators data we cleaned in step 1 and create a neural net to create a model see how great an approximation our model at predicting development was. If we receive a high enough accuracy (90%+) we can continue.
"""

dataset = combined_df.values
X = dataset[:,0:12]
Y = dataset[:,12]
X

min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)
X_scale

X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)
X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)
print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)

"""We first train a simple Logisic Regression on our completely cleaned dataset. After training the data and fitting it on a train set, we see that we have an accuarcny of 0.94 on the test set. """

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

logistic = LogisticRegression()
logistic.fit(X_train, Y_train)
logistic_pred = logistic.predict(X_test)
print("Accuracy: ", metrics.accuracy_score(Y_test, logistic_pred))

"""We then create a neural net to hopefully find a more accurate method to fit the data. We have three Dense layers in this Keras Neural Net which we process and train below. We use the relu activation parameter on the first two layers and the sigmoid parameter on the last one."""

model = Sequential([
    Dense(32, activation='relu', input_shape=(12,)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid'),
])

model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])

hist = model.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))

model.evaluate(X_test, Y_test)[1]

"""Our accuracy for the neural net is above the observed accuracy for the Logistic Regression, and therefore we will use this neural net to predict future values. We show plots of the accuracy and loss variables as we trained the 100 epochs above. Now we go and generate future values for our indicators from years 2020 to 2030. """

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

India_df = full_df[full_df['Country'] == 'India']
India_rural = India_df[['Year', 'rural']].reset_index()
India_rural = India_rural.drop(columns= {'index'})
NaN = np.nan
India_rural.loc[len(India_rural.index)] = [2020, 0]
India_rural.loc[len(India_rural.index)] = [2021, 0]
India_rural.loc[len(India_rural.index)] = [2022, 0]
India_rural.loc[len(India_rural.index)] = [2023, 0]
India_rural.loc[len(India_rural.index)] = [2024, 0]
India_rural.loc[len(India_rural.index)] = [2025, 0]
India_rural.loc[len(India_rural.index)] = [2026, 0]
India_rural.loc[len(India_rural.index)] = [2027, 0]
India_rural.loc[len(India_rural.index)] = [2028, 0]
India_rural.loc[len(India_rural.index)] = [2029, 0]
India_rural.loc[len(India_rural.index)] = [2030, 0]
India_rural['Year'] = pd.to_datetime(India_rural['Year'], format='%Y')
India_rural = India_rural.set_index('Year')
India_rural

#In the next several sells, we will explore different methods of forecast our data from 2020-2030. We find that Holt-Linear, Holt-Winter, and ARIMA are teh most efficent methods of doing this.

India_rural.head(34).plot(figsize=(15, 6))
plt.show()

India_rural
lst = list(range(0,46))
arr = np.array(lst)
India_rural['years_from_start'] = arr
India_rural

x = India_rural['years_from_start'].values.reshape(-1, 1)
y = India_rural['rural'].values

# linear_model = linear_model.LinearRegression().fit(x, y)
# linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
# LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
# linear_model.predict([[1]])

train=India_rural[0:35] 
test=India_rural[34:]

y_hat_avg = test.copy()
y_hat_avg['moving_avg_forecast'] = train['rural'].rolling(2).mean().iloc[-1]
plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='Train')
plt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')
plt.legend(loc='best')
plt.show()

rms = sqrt(mean_squared_error(test.rural, y_hat_avg.moving_avg_forecast))
print(rms)

y_hat_avg = test.copy()
fit2 = SimpleExpSmoothing(np.asarray(train['rural'])).fit(smoothing_level=0.6,optimized=False)
y_hat_avg['SES'] = fit2.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='Train')
plt.plot(y_hat_avg['SES'], label='SES')
plt.legend(loc='best')
plt.show()

rms = sqrt(mean_squared_error(test.rural, y_hat_avg.SES))
print(rms)

sm.tsa.seasonal_decompose(train.rural).plot()
result = sm.tsa.stattools.adfuller(train.rural)
plt.show()

y_hat_avg = test.copy()

fit1 = Holt(np.asarray(train['rural'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit1.forecast(len(test))

plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='Train')
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
plt.show()

rms = sqrt(mean_squared_error(test.rural, y_hat_avg.Holt_linear))
print(rms)

y_hat_avg = test.copy()
fit1 = ExponentialSmoothing(np.asarray(train['rural']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot( train['rural'], label='Train')
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
plt.show()

rms = sqrt(mean_squared_error(test.rural, y_hat_avg.Holt_Winter))
print(rms)

y_hat_avg = test.copy()
fit1 = sm.tsa.statespace.SARIMAX(train.rural, order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit1.predict(start="2019-01-01", end="2030-01-01", dynamic=True)
plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='Train')
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
y_hat_avg['SARIMA']

rms = sqrt(mean_squared_error(test.rural, y_hat_avg.SARIMA))
print(rms)

y_hat_avg = y_hat_avg.iloc[1: , :]
y_hat_avg = y_hat_avg.drop(columns={'rural'})
y_hat_avg = y_hat_avg.rename(columns={'SARIMA': 'rural'})
y_hat_avg['years_from_start'] = y_hat_avg['years_from_start'] + 1985
y_hat_avg = y_hat_avg.reset_index()
y_hat_avg = y_hat_avg.drop(columns={'Year'})
y_hat_avg = y_hat_avg.rename(columns={'years_from_start': 'Year'})
y_hat_avg


#new_df = pd.concat([India_rural, y_hat_avg], axis=0)
#new_df



India_df = full_df[full_df['Country'] == 'India']
India_immunization = India_df[['Year', 'immunization']].reset_index()
India_immunization = India_immunization.drop(columns= {'index'})
NaN = np.nan
India_immunization.loc[len(India_immunization.index)] = [2020, 0]
India_immunization.loc[len(India_immunization.index)] = [2021, 0]
India_immunization.loc[len(India_immunization.index)] = [2022, 0]
India_immunization.loc[len(India_immunization.index)] = [2023, 0]
India_immunization.loc[len(India_immunization.index)] = [2024, 0]
India_immunization.loc[len(India_immunization.index)] = [2025, 0]
India_immunization.loc[len(India_immunization.index)] = [2026, 0]
India_immunization.loc[len(India_immunization.index)] = [2027, 0]
India_immunization.loc[len(India_immunization.index)] = [2028, 0]
India_immunization.loc[len(India_immunization.index)] = [2029, 0]
India_immunization.loc[len(India_immunization.index)] = [2030, 0]
India_immunization['Year'] = pd.to_datetime(India_immunization['Year'], format='%Y')
India_immunization = India_immunization.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_immunization['years_from_start'] = arr
x = India_immunization['years_from_start'].values.reshape(-1, 1)
y = India_immunization['immunization'].values
train=India_immunization[0:35] 
test=India_immunization[34:]

dd= np.asarray(train.immunization)

y_hat_avg = test.copy()
fit1 = sm.tsa.statespace.SARIMAX(train.immunization, order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit1.predict(start="2020-01-01", end="2030-01-01", dynamic=True)
plt.figure(figsize=(16,8))
plt.plot( train['immunization'], label='Train')
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
y_hat_avg['SARIMA']

y_hat_avg = y_hat_avg.iloc[1: , :]
y_hat_avg = y_hat_avg.drop(columns={'immunization'})
y_hat_avg = y_hat_avg.rename(columns={'SARIMA': 'immunization'})
y_hat_avg['years_from_start'] = y_hat_avg['years_from_start'] + 1985
y_hat_avg = y_hat_avg.reset_index()
y_hat_avg = y_hat_avg.drop(columns={'Year'})
y_hat_avg = y_hat_avg.rename(columns={'years_from_start': 'Year'})
y_hat_avg

India_df = full_df[full_df['Country'] == 'India']
India_immunization = India_df[['Year', 'immunization']].reset_index()
India_immunization = India_immunization.drop(columns= {'index'})
NaN = np.nan
India_immunization.loc[len(India_immunization.index)] = [2020, 0]
India_immunization.loc[len(India_immunization.index)] = [2021, 0]
India_immunization.loc[len(India_immunization.index)] = [2022, 0]
India_immunization.loc[len(India_immunization.index)] = [2023, 0]
India_immunization.loc[len(India_immunization.index)] = [2024, 0]
India_immunization.loc[len(India_immunization.index)] = [2025, 0]
India_immunization.loc[len(India_immunization.index)] = [2026, 0]
India_immunization.loc[len(India_immunization.index)] = [2027, 0]
India_immunization.loc[len(India_immunization.index)] = [2028, 0]
India_immunization.loc[len(India_immunization.index)] = [2029, 0]
India_immunization.loc[len(India_immunization.index)] = [2030, 0]
India_immunization['Year'] = pd.to_datetime(India_immunization['Year'], format='%Y')
India_immunization = India_immunization.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_immunization['years_from_start'] = arr
x = India_immunization['years_from_start'].values.reshape(-1, 1)
y = India_immunization['immunization'].values
train=India_immunization[0:35] 
test=India_immunization[34:]

dd= np.asarray(train.immunization)

y_hat_avg_immunization = test.copy()
fit1 = ExponentialSmoothing(np.asarray(train['immunization']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg_immunization['Holt_Winter'] = fit1.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot( train['immunization'], label='Train')
plt.plot(y_hat_avg_immunization['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
plt.show()
y_hat_avg_immunization['Holt_Winter']

y_hat_avg_immunization = y_hat_avg_immunization.iloc[1: , :]
y_hat_avg_immunization = y_hat_avg_immunization.drop(columns={'immunization'})
y_hat_avg_immunization = y_hat_avg_immunization.rename(columns={'Holt_Winter': 'immunization'})
y_hat_avg_immunization['years_from_start'] = y_hat_avg_immunization['years_from_start'] + 1985
y_hat_avg_immunization = y_hat_avg_immunization.reset_index()
y_hat_avg_immunization = y_hat_avg_immunization.drop(columns={'Year'})
y_hat_avg_immunization = y_hat_avg_immunization.rename(columns={'years_from_start': 'Year'})
y_hat_avg_immunization

#new_df = pd.concat([India_immunization, y_hat_avg], axis=0)
#new_df



"""Here we first run a Linear Regression to see if it can accurately map our train data to the test data we already have. We clearly observe for most countries this model will not be adequate (very high RMSE) to plot our data. We then use more accurate and complex methods such as Exponentual Smoothing, Holt Linear, Holt Winter and SARIMA which are all proven models to fit time series data. We will calculate the RSME values of each regression and then only use the smallest to actually calculate the future data points we need per indicator (data from 2020 to 2030), which we do in the next step!

Rural Data Forecast
"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_rural = India_df[['Year', 'rural']].reset_index()
India_rural = India_rural.drop(columns= {'index'})
India_rural = India_rural.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_rural['years_from_start'] = arr
x = India_rural['years_from_start'].values.reshape(-1, 1)
y = India_rural['rural'].values
train=India_rural[0:28] 
test=India_rural[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='Train')
plt.plot(test['rural'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['rural'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test.rural, y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['rural']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test.rural, y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train.rural, order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test.rural, y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_rural = India_df[['Year', 'rural']].reset_index()
India_rural = India_rural.drop(columns= {'index'})
NaN = np.nan
India_rural.loc[len(India_rural.index)] = [2020, 0]
India_rural.loc[len(India_rural.index)] = [2021, 0]
India_rural.loc[len(India_rural.index)] = [2022, 0]
India_rural.loc[len(India_rural.index)] = [2023, 0]
India_rural.loc[len(India_rural.index)] = [2024, 0]
India_rural.loc[len(India_rural.index)] = [2025, 0]
India_rural.loc[len(India_rural.index)] = [2026, 0]
India_rural.loc[len(India_rural.index)] = [2027, 0]
India_rural.loc[len(India_rural.index)] = [2028, 0]
India_rural.loc[len(India_rural.index)] = [2029, 0]
India_rural.loc[len(India_rural.index)] = [2030, 0]
India_rural['Year'] = pd.to_datetime(India_rural['Year'], format='%Y')
India_rural = India_rural.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_rural['years_from_start'] = arr
x = India_rural['years_from_start'].values.reshape(-1, 1)
y = India_rural['rural'].values
train=India_rural[0:35] 
test=India_rural[34:]

dd= np.asarray(train.rural)

y_hat_avg_rural = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train.rural, order=(2, 1, 4)).fit()
y_hat_avg_rural['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['rural'], label='train')
plt.plot(y_hat_avg_rural['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_rural['SARIMA']

y_hat_avg_rural = y_hat_avg_rural.iloc[1: , :]
y_hat_avg_rural = y_hat_avg_rural.drop(columns={'rural'})
y_hat_avg_rural = y_hat_avg_rural.rename(columns={'SARIMA': 'rural'})
y_hat_avg_rural['years_from_start'] = y_hat_avg_rural['years_from_start'] + 1985
y_hat_avg_rural = y_hat_avg_rural.reset_index()
y_hat_avg_rural = y_hat_avg_rural.drop(columns={'Year'})
y_hat_avg_rural = y_hat_avg_rural.rename(columns={'years_from_start': 'Year'})
y_hat_avg_rural

"""CO2 emissions (metric tons per capita) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_c02 = India_df[['Year', 'CO2 emissions (metric tons per capita)']].reset_index()
India_c02 = India_c02.drop(columns= {'index'})
India_c02 = India_c02.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_c02['years_from_start'] = arr
x = India_c02['years_from_start'].values.reshape(-1, 1)
y = India_c02['CO2 emissions (metric tons per capita)'].values
train=India_c02[0:28] 
test=India_c02[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['CO2 emissions (metric tons per capita)'], label='Train')
plt.plot(test['CO2 emissions (metric tons per capita)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['CO2 emissions (metric tons per capita)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['CO2 emissions (metric tons per capita)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['CO2 emissions (metric tons per capita)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['CO2 emissions (metric tons per capita)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['CO2 emissions (metric tons per capita)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['CO2 emissions (metric tons per capita)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_c02 = India_df[['Year', 'CO2 emissions (metric tons per capita)']].reset_index()
India_c02 = India_c02.drop(columns= {'index'})
NaN = np.nan
India_c02.loc[len(India_c02.index)] = [2020, 0]
India_c02.loc[len(India_c02.index)] = [2021, 0]
India_c02.loc[len(India_c02.index)] = [2022, 0]
India_c02.loc[len(India_c02.index)] = [2023, 0]
India_c02.loc[len(India_c02.index)] = [2024, 0]
India_c02.loc[len(India_c02.index)] = [2025, 0]
India_c02.loc[len(India_c02.index)] = [2026, 0]
India_c02.loc[len(India_c02.index)] = [2027, 0]
India_c02.loc[len(India_c02.index)] = [2028, 0]
India_c02.loc[len(India_c02.index)] = [2029, 0]
India_c02.loc[len(India_c02.index)] = [2030, 0]
India_c02['Year'] = pd.to_datetime(India_c02['Year'], format='%Y')
India_c02 = India_c02.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_c02['years_from_start'] = arr
x = India_c02['years_from_start'].values.reshape(-1, 1)
y = India_c02['CO2 emissions (metric tons per capita)'].values
train=India_c02[0:35] 
test=India_c02[34:]

dd= np.asarray(train['CO2 emissions (metric tons per capita)'])

y_hat_avg_c02 = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['CO2 emissions (metric tons per capita)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg_c02['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['CO2 emissions (metric tons per capita)'], label='train')
plt.plot(y_hat_avg_c02['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
plt.show()
y_hat_avg_c02['Holt_Winter']



y_hat_avg_c02 = y_hat_avg_c02.iloc[1: , :]
y_hat_avg_c02 = y_hat_avg_c02.drop(columns={'CO2 emissions (metric tons per capita)'})
y_hat_avg_c02 = y_hat_avg_c02.rename(columns={'Holt_Winter': 'CO2 emissions (metric tons per capita)'})
y_hat_avg_c02['years_from_start'] = y_hat_avg_c02['years_from_start'] + 1985
y_hat_avg_c02 = y_hat_avg_c02.reset_index()
y_hat_avg_c02 = y_hat_avg_c02.drop(columns={'Year'})
y_hat_avg_c02 = y_hat_avg_c02.rename(columns={'years_from_start': 'Year'})
y_hat_avg_c02

"""Renewable energy consumption (% of total final energy consumption) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_energy = India_df[['Year', 'Renewable energy consumption (% of total final energy consumption)']].reset_index()
India_energy = India_energy.drop(columns= {'index'})
India_energy = India_energy.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_energy['years_from_start'] = arr
x = India_energy['years_from_start'].values.reshape(-1, 1)
y = India_energy['Renewable energy consumption (% of total final energy consumption)'].values
train=India_energy[0:28] 
test=India_energy[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Renewable energy consumption (% of total final energy consumption)'], label='Train')
plt.plot(test['Renewable energy consumption (% of total final energy consumption)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Renewable energy consumption (% of total final energy consumption)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Renewable energy consumption (% of total final energy consumption)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Renewable energy consumption (% of total final energy consumption)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Renewable energy consumption (% of total final energy consumption)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Renewable energy consumption (% of total final energy consumption)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Renewable energy consumption (% of total final energy consumption)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_energy = India_df[['Year', 'Renewable energy consumption (% of total final energy consumption)']].reset_index()
India_energy = India_energy.drop(columns= {'index'})
NaN = np.nan
India_energy.loc[len(India_energy.index)] = [2020, 0]
India_energy.loc[len(India_energy.index)] = [2021, 0]
India_energy.loc[len(India_energy.index)] = [2022, 0]
India_energy.loc[len(India_energy.index)] = [2023, 0]
India_energy.loc[len(India_energy.index)] = [2024, 0]
India_energy.loc[len(India_energy.index)] = [2025, 0]
India_energy.loc[len(India_energy.index)] = [2026, 0]
India_energy.loc[len(India_energy.index)] = [2027, 0]
India_energy.loc[len(India_energy.index)] = [2028, 0]
India_energy.loc[len(India_energy.index)] = [2029, 0]
India_energy.loc[len(India_energy.index)] = [2030, 0]
India_energy['Year'] = pd.to_datetime(India_energy['Year'], format='%Y')
India_energy = India_energy.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_energy['years_from_start'] = arr
x = India_energy['years_from_start'].values.reshape(-1, 1)
y = India_energy['Renewable energy consumption (% of total final energy consumption)'].values
train=India_energy[0:35] 
test=India_energy[34:]

dd= np.asarray(train['Renewable energy consumption (% of total final energy consumption)'])

y_hat_avg_energy = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Renewable energy consumption (% of total final energy consumption)'], order=(2, 1, 4)).fit()
y_hat_avg_energy['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Renewable energy consumption (% of total final energy consumption)'], label='train')
plt.plot(y_hat_avg_energy['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_energy['SARIMA']

y_hat_avg_energy = y_hat_avg_energy.iloc[1: , :]
y_hat_avg_energy = y_hat_avg_energy.drop(columns={'Renewable energy consumption (% of total final energy consumption)'})
y_hat_avg_energy = y_hat_avg_energy.rename(columns={'SARIMA': 'Renewable energy consumption (% of total final energy consumption)'})
y_hat_avg_energy['years_from_start'] = y_hat_avg_energy['years_from_start'] + 1985
y_hat_avg_energy = y_hat_avg_energy.reset_index()
y_hat_avg_energy = y_hat_avg_energy.drop(columns={'Year'})
y_hat_avg_energy = y_hat_avg_energy.rename(columns={'years_from_start': 'Year'})
y_hat_avg_energy

"""Birth rate, crude (per 1,000 people) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_birth = India_df[['Year', 'Birth rate, crude (per 1,000 people)']].reset_index()
India_birth = India_birth.drop(columns= {'index'})
India_birth = India_birth.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_birth['years_from_start'] = arr
x = India_birth['years_from_start'].values.reshape(-1, 1)
y = India_birth['Birth rate, crude (per 1,000 people)'].values
train=India_birth[0:28] 
test=India_birth[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Birth rate, crude (per 1,000 people)'], label='Train')
plt.plot(test['Birth rate, crude (per 1,000 people)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Birth rate, crude (per 1,000 people)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Birth rate, crude (per 1,000 people)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Birth rate, crude (per 1,000 people)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Birth rate, crude (per 1,000 people)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Birth rate, crude (per 1,000 people)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Birth rate, crude (per 1,000 people)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_birth = India_df[['Year', 'Birth rate, crude (per 1,000 people)']].reset_index()
India_birth = India_birth.drop(columns= {'index'})
NaN = np.nan
India_birth.loc[len(India_birth.index)] = [2020, 0]
India_birth.loc[len(India_birth.index)] = [2021, 0]
India_birth.loc[len(India_birth.index)] = [2022, 0]
India_birth.loc[len(India_birth.index)] = [2023, 0]
India_birth.loc[len(India_birth.index)] = [2024, 0]
India_birth.loc[len(India_birth.index)] = [2025, 0]
India_birth.loc[len(India_birth.index)] = [2026, 0]
India_birth.loc[len(India_birth.index)] = [2027, 0]
India_birth.loc[len(India_birth.index)] = [2028, 0]
India_birth.loc[len(India_birth.index)] = [2029, 0]
India_birth.loc[len(India_birth.index)] = [2030, 0]
India_birth['Year'] = pd.to_datetime(India_birth['Year'], format='%Y')
India_birth = India_birth.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_birth['years_from_start'] = arr
x = India_birth['years_from_start'].values.reshape(-1, 1)
y = India_birth['Birth rate, crude (per 1,000 people)'].values
train=India_birth[0:35] 
test=India_birth[34:]

dd= np.asarray(train['Birth rate, crude (per 1,000 people)'])

y_hat_avg_birth = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Birth rate, crude (per 1,000 people)'], order=(2, 1, 4)).fit()
y_hat_avg_birth['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Birth rate, crude (per 1,000 people)'], label='train')
plt.plot(y_hat_avg_birth['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_birth['SARIMA']

y_hat_avg_birth = y_hat_avg_birth.iloc[1: , :]
y_hat_avg_birth = y_hat_avg_birth.drop(columns={'Birth rate, crude (per 1,000 people)'})
y_hat_avg_birth = y_hat_avg_birth.rename(columns={'SARIMA': 'Birth rate, crude (per 1,000 people)'})
y_hat_avg_birth['years_from_start'] = y_hat_avg_birth['years_from_start'] + 1985
y_hat_avg_birth = y_hat_avg_birth.reset_index()
y_hat_avg_birth = y_hat_avg_birth.drop(columns={'Year'})
y_hat_avg_birth = y_hat_avg_birth.rename(columns={'years_from_start': 'Year'})
y_hat_avg_birth

"""Immunization, measles (% of children ages 12-23 months) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_measles = India_df[['Year', 'Immunization, measles (% of children ages 12-23 months)']].reset_index()
India_measles = India_measles.drop(columns= {'index'})
India_measles = India_measles.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_measles['years_from_start'] = arr
x = India_measles['years_from_start'].values.reshape(-1, 1)
y = India_measles['Immunization, measles (% of children ages 12-23 months)'].values
train=India_measles[0:28] 
test=India_measles[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Immunization, measles (% of children ages 12-23 months)'], label='Train')
plt.plot(test['Immunization, measles (% of children ages 12-23 months)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Immunization, measles (% of children ages 12-23 months)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Immunization, measles (% of children ages 12-23 months)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Immunization, measles (% of children ages 12-23 months)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Immunization, measles (% of children ages 12-23 months)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Immunization, measles (% of children ages 12-23 months)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Immunization, measles (% of children ages 12-23 months)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_measles = India_df[['Year', 'Immunization, measles (% of children ages 12-23 months)']].reset_index()
India_measles = India_measles.drop(columns= {'index'})
NaN = np.nan
India_measles.loc[len(India_measles.index)] = [2020, 0]
India_measles.loc[len(India_measles.index)] = [2021, 0]
India_measles.loc[len(India_measles.index)] = [2022, 0]
India_measles.loc[len(India_measles.index)] = [2023, 0]
India_measles.loc[len(India_measles.index)] = [2024, 0]
India_measles.loc[len(India_measles.index)] = [2025, 0]
India_measles.loc[len(India_measles.index)] = [2026, 0]
India_measles.loc[len(India_measles.index)] = [2027, 0]
India_measles.loc[len(India_measles.index)] = [2028, 0]
India_measles.loc[len(India_measles.index)] = [2029, 0]
India_measles.loc[len(India_measles.index)] = [2030, 0]
India_measles['Year'] = pd.to_datetime(India_measles['Year'], format='%Y')
India_measles = India_measles.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_measles['years_from_start'] = arr
x = India_measles['years_from_start'].values.reshape(-1, 1)
y = India_measles['Immunization, measles (% of children ages 12-23 months)'].values
train=India_measles[0:35] 
test=India_measles[34:]

dd= np.asarray(train['Immunization, measles (% of children ages 12-23 months)'])

y_hat_avg_measles = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Immunization, measles (% of children ages 12-23 months)'], order=(2, 1, 4)).fit()
y_hat_avg_measles['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Immunization, measles (% of children ages 12-23 months)'], label='train')
plt.plot(y_hat_avg_measles['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_measles['SARIMA']

y_hat_avg_measles = y_hat_avg_measles.iloc[1: , :]
y_hat_avg_measles = y_hat_avg_measles.drop(columns={'Immunization, measles (% of children ages 12-23 months)'})
y_hat_avg_measles = y_hat_avg_measles.rename(columns={'SARIMA': 'Immunization, measles (% of children ages 12-23 months)'})
y_hat_avg_measles['years_from_start'] = y_hat_avg_measles['years_from_start'] + 1985
y_hat_avg_measles = y_hat_avg_measles.reset_index()
y_hat_avg_measles = y_hat_avg_measles.drop(columns={'Year'})
y_hat_avg_measles = y_hat_avg_measles.rename(columns={'years_from_start': 'Year'})
y_hat_avg_measles

"""c

Life expectancy at birth, total (years) Future Predictions
"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_life = India_df[['Year', 'Life expectancy at birth, total (years)']].reset_index()
India_life = India_life.drop(columns= {'index'})
India_life = India_life.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_life['years_from_start'] = arr
x = India_life['years_from_start'].values.reshape(-1, 1)
y = India_life['Life expectancy at birth, total (years)'].values
train=India_life[0:28] 
test=India_life[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Life expectancy at birth, total (years)'], label='Train')
plt.plot(test['Life expectancy at birth, total (years)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Life expectancy at birth, total (years)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Life expectancy at birth, total (years)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Life expectancy at birth, total (years)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Life expectancy at birth, total (years)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Life expectancy at birth, total (years)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Life expectancy at birth, total (years)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_life = India_df[['Year', 'Life expectancy at birth, total (years)']].reset_index()
India_life = India_life.drop(columns= {'index'})
NaN = np.nan
India_life.loc[len(India_life.index)] = [2020, 0]
India_life.loc[len(India_life.index)] = [2021, 0]
India_life.loc[len(India_life.index)] = [2022, 0]
India_life.loc[len(India_life.index)] = [2023, 0]
India_life.loc[len(India_life.index)] = [2024, 0]
India_life.loc[len(India_life.index)] = [2025, 0]
India_life.loc[len(India_life.index)] = [2026, 0]
India_life.loc[len(India_life.index)] = [2027, 0]
India_life.loc[len(India_life.index)] = [2028, 0]
India_life.loc[len(India_life.index)] = [2029, 0]
India_life.loc[len(India_life.index)] = [2030, 0]
India_life['Year'] = pd.to_datetime(India_life['Year'], format='%Y')
India_life = India_life.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_life['years_from_start'] = arr
x = India_life['years_from_start'].values.reshape(-1, 1)
y = India_life['Life expectancy at birth, total (years)'].values
train=India_life[0:35] 
test=India_life[34:]

dd= np.asarray(train['Life expectancy at birth, total (years)'])

y_hat_avg_life = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Life expectancy at birth, total (years)'], order=(2, 1, 4)).fit()
y_hat_avg_life['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Life expectancy at birth, total (years)'], label='train')
plt.plot(y_hat_avg_life['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_life['SARIMA']

y_hat_avg_life = y_hat_avg_life.iloc[1: , :]
y_hat_avg_life = y_hat_avg_life.drop(columns={'Life expectancy at birth, total (years)'})
y_hat_avg_life = y_hat_avg_life.rename(columns={'SARIMA': 'Life expectancy at birth, total (years)'})
y_hat_avg_life['years_from_start'] = y_hat_avg_life['years_from_start'] + 1985
y_hat_avg_life = y_hat_avg_life.reset_index()
y_hat_avg_life = y_hat_avg_life.drop(columns={'Year'})
y_hat_avg_life = y_hat_avg_life.rename(columns={'years_from_start': 'Year'})
y_hat_avg_life

"""Population growth (annual %)	Adjusted savings: education expenditure (% of GNI)	GDP per capita (current US$)	Individuals using the Internet (% of population)	Developed?

Mortality rate, under-5 (per 1,000 live births) Future Predictions
"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_mortality = India_df[['Year', 'Mortality rate, under-5 (per 1,000 live births)']].reset_index()
India_mortality = India_mortality.drop(columns= {'index'})
India_mortality = India_mortality.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_mortality['years_from_start'] = arr
x = India_mortality['years_from_start'].values.reshape(-1, 1)
y = India_mortality['Mortality rate, under-5 (per 1,000 live births)'].values
train=India_mortality[0:28] 
test=India_mortality[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Mortality rate, under-5 (per 1,000 live births)'], label='Train')
plt.plot(test['Mortality rate, under-5 (per 1,000 live births)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Mortality rate, under-5 (per 1,000 live births)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Mortality rate, under-5 (per 1,000 live births)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Mortality rate, under-5 (per 1,000 live births)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Mortality rate, under-5 (per 1,000 live births)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Mortality rate, under-5 (per 1,000 live births)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Mortality rate, under-5 (per 1,000 live births)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_mortality = India_df[['Year', 'Mortality rate, under-5 (per 1,000 live births)']].reset_index()
India_mortality = India_mortality.drop(columns= {'index'})
NaN = np.nan
India_mortality.loc[len(India_mortality.index)] = [2020, 0]
India_mortality.loc[len(India_mortality.index)] = [2021, 0]
India_mortality.loc[len(India_mortality.index)] = [2022, 0]
India_mortality.loc[len(India_mortality.index)] = [2023, 0]
India_mortality.loc[len(India_mortality.index)] = [2024, 0]
India_mortality.loc[len(India_mortality.index)] = [2025, 0]
India_mortality.loc[len(India_mortality.index)] = [2026, 0]
India_mortality.loc[len(India_mortality.index)] = [2027, 0]
India_mortality.loc[len(India_mortality.index)] = [2028, 0]
India_mortality.loc[len(India_mortality.index)] = [2029, 0]
India_mortality.loc[len(India_mortality.index)] = [2030, 0]
India_mortality['Year'] = pd.to_datetime(India_mortality['Year'], format='%Y')
India_mortality = India_mortality.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_mortality['years_from_start'] = arr
x = India_mortality['years_from_start'].values.reshape(-1, 1)
y = India_mortality['Mortality rate, under-5 (per 1,000 live births)'].values
train=India_mortality[0:35] 
test=India_mortality[34:]

dd= np.asarray(train['Mortality rate, under-5 (per 1,000 live births)'])

y_hat_avg_mortality = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Mortality rate, under-5 (per 1,000 live births)'], order=(2, 1, 4)).fit()
y_hat_avg_mortality['SARIMA'] = fit_arima.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Mortality rate, under-5 (per 1,000 live births)'], label='train')
plt.plot(y_hat_avg_mortality['SARIMA'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_mortality['SARIMA']

y_hat_avg_mortality = y_hat_avg_mortality.iloc[1: , :]
y_hat_avg_mortality = y_hat_avg_mortality.drop(columns={'Mortality rate, under-5 (per 1,000 live births)'})
y_hat_avg_mortality = y_hat_avg_mortality.rename(columns={'SARIMA': 'Mortality rate, under-5 (per 1,000 live births)'})
y_hat_avg_mortality['years_from_start'] = y_hat_avg_mortality['years_from_start'] + 1985
y_hat_avg_mortality = y_hat_avg_mortality.reset_index()
y_hat_avg_mortality = y_hat_avg_mortality.drop(columns={'Year'})
y_hat_avg_mortality = y_hat_avg_mortality.rename(columns={'years_from_start': 'Year'})
y_hat_avg_mortality

"""Population growth (annual %) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_population = India_df[['Year', 'Population growth (annual %)']].reset_index()
India_population = India_population.drop(columns= {'index'})
India_population = India_population.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_population['years_from_start'] = arr
x = India_population['years_from_start'].values.reshape(-1, 1)
y = India_population['Population growth (annual %)'].values
train=India_population[0:28] 
test=India_population[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Population growth (annual %)'], label='Train')
plt.plot(test['Population growth (annual %)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Population growth (annual %)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Population growth (annual %)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Population growth (annual %)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Population growth (annual %)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Population growth (annual %)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Population growth (annual %)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_population = India_df[['Year', 'Population growth (annual %)']].reset_index()
India_population = India_population.drop(columns= {'index'})
NaN = np.nan
India_population.loc[len(India_population.index)] = [2020, 0]
India_population.loc[len(India_population.index)] = [2021, 0]
India_population.loc[len(India_population.index)] = [2022, 0]
India_population.loc[len(India_population.index)] = [2023, 0]
India_population.loc[len(India_population.index)] = [2024, 0]
India_population.loc[len(India_population.index)] = [2025, 0]
India_population.loc[len(India_population.index)] = [2026, 0]
India_population.loc[len(India_population.index)] = [2027, 0]
India_population.loc[len(India_population.index)] = [2028, 0]
India_population.loc[len(India_population.index)] = [2029, 0]
India_population.loc[len(India_population.index)] = [2030, 0]
India_population['Year'] = pd.to_datetime(India_population['Year'], format='%Y')
India_population = India_population.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_population['years_from_start'] = arr
x = India_population['years_from_start'].values.reshape(-1, 1)
y = India_population['Population growth (annual %)'].values
train=India_population[0:35] 
test=India_population[34:]

dd= np.asarray(train['Population growth (annual %)'])

y_hat_avg_population = test.copy()
fit_holt_linear = Holt(np.asarray(train['Population growth (annual %)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg_population['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Population growth (annual %)'], label='train')
plt.plot(y_hat_avg_population['Holt_linear'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_population['Holt_linear']

y_hat_avg_population = y_hat_avg_population.iloc[1: , :]
y_hat_avg_population = y_hat_avg_population.drop(columns={'Population growth (annual %)'})
y_hat_avg_population = y_hat_avg_population.rename(columns={'Holt_linear': 'Population growth (annual %)'})
y_hat_avg_population['years_from_start'] = y_hat_avg_population['years_from_start'] + 1985
y_hat_avg_population = y_hat_avg_population.reset_index()
y_hat_avg_population = y_hat_avg_population.drop(columns={'Year'})
y_hat_avg_population = y_hat_avg_population.rename(columns={'years_from_start': 'Year'})
y_hat_avg_population

"""Adjusted savings: education expenditure (% of GNI) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_education = India_df[['Year', 'Adjusted savings: education expenditure (% of GNI)']].reset_index()
India_education = India_education.drop(columns= {'index'})
India_education = India_education.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_education['years_from_start'] = arr
x = India_education['years_from_start'].values.reshape(-1, 1)
y = India_education['Adjusted savings: education expenditure (% of GNI)'].values
train=India_education[0:28] 
test=India_education[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Adjusted savings: education expenditure (% of GNI)'], label='Train')
plt.plot(test['Adjusted savings: education expenditure (% of GNI)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Adjusted savings: education expenditure (% of GNI)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Adjusted savings: education expenditure (% of GNI)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Adjusted savings: education expenditure (% of GNI)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Adjusted savings: education expenditure (% of GNI)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Adjusted savings: education expenditure (% of GNI)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Adjusted savings: education expenditure (% of GNI)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_education = India_df[['Year', 'Adjusted savings: education expenditure (% of GNI)']].reset_index()
India_education = India_education.drop(columns= {'index'})
NaN = np.nan
India_education.loc[len(India_education.index)] = [2020, 0]
India_education.loc[len(India_education.index)] = [2021, 0]
India_education.loc[len(India_education.index)] = [2022, 0]
India_education.loc[len(India_education.index)] = [2023, 0]
India_education.loc[len(India_education.index)] = [2024, 0]
India_education.loc[len(India_education.index)] = [2025, 0]
India_education.loc[len(India_education.index)] = [2026, 0]
India_education.loc[len(India_education.index)] = [2027, 0]
India_education.loc[len(India_education.index)] = [2028, 0]
India_education.loc[len(India_education.index)] = [2029, 0]
India_education.loc[len(India_education.index)] = [2030, 0]
India_education['Year'] = pd.to_datetime(India_education['Year'], format='%Y')
India_education = India_education.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_education['years_from_start'] = arr
x = India_education['years_from_start'].values.reshape(-1, 1)
y = India_education['Adjusted savings: education expenditure (% of GNI)'].values
train=India_education[0:35] 
test=India_education[34:]

dd= np.asarray(train['Adjusted savings: education expenditure (% of GNI)'])

y_hat_avg_education = test.copy()
fit_holt_linear = Holt(np.asarray(train['Adjusted savings: education expenditure (% of GNI)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg_education['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Adjusted savings: education expenditure (% of GNI)'], label='train')
plt.plot(y_hat_avg_education['Holt_linear'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_education['Holt_linear']

y_hat_avg_education = y_hat_avg_education.iloc[1: , :]
y_hat_avg_education = y_hat_avg_education.drop(columns={'Adjusted savings: education expenditure (% of GNI)'})
y_hat_avg_education = y_hat_avg_education.rename(columns={'Holt_linear': 'Adjusted savings: education expenditure (% of GNI)'})
y_hat_avg_education['years_from_start'] = y_hat_avg_education['years_from_start'] + 1985
y_hat_avg_education = y_hat_avg_education.reset_index()
y_hat_avg_education = y_hat_avg_education.drop(columns={'Year'})
y_hat_avg_education = y_hat_avg_education.rename(columns={'years_from_start': 'Year'})
y_hat_avg_education

"""GDP per capita (current US$) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_gdp = India_df[['Year', 'GDP per capita (current US$)']].reset_index()
India_gdp = India_gdp.drop(columns= {'index'})
India_gdp = India_gdp.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_gdp['years_from_start'] = arr
x = India_gdp['years_from_start'].values.reshape(-1, 1)
y = India_gdp['GDP per capita (current US$)'].values
train=India_gdp[0:28] 
test=India_gdp[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['GDP per capita (current US$)'], label='Train')
plt.plot(test['GDP per capita (current US$)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['GDP per capita (current US$)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['GDP per capita (current US$)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['GDP per capita (current US$)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['GDP per capita (current US$)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['GDP per capita (current US$)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['GDP per capita (current US$)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_gdp = India_df[['Year', 'GDP per capita (current US$)']].reset_index()
India_gdp = India_gdp.drop(columns= {'index'})
NaN = np.nan
India_gdp.loc[len(India_gdp.index)] = [2020, 0]
India_gdp.loc[len(India_gdp.index)] = [2021, 0]
India_gdp.loc[len(India_gdp.index)] = [2022, 0]
India_gdp.loc[len(India_gdp.index)] = [2023, 0]
India_gdp.loc[len(India_gdp.index)] = [2024, 0]
India_gdp.loc[len(India_gdp.index)] = [2025, 0]
India_gdp.loc[len(India_gdp.index)] = [2026, 0]
India_gdp.loc[len(India_gdp.index)] = [2027, 0]
India_gdp.loc[len(India_gdp.index)] = [2028, 0]
India_gdp.loc[len(India_gdp.index)] = [2029, 0]
India_gdp.loc[len(India_gdp.index)] = [2030, 0]
India_gdp['Year'] = pd.to_datetime(India_gdp['Year'], format='%Y')
India_gdp = India_gdp.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_gdp['years_from_start'] = arr
x = India_gdp['years_from_start'].values.reshape(-1, 1)
y = India_gdp['GDP per capita (current US$)'].values
train=India_gdp[0:35] 
test=India_gdp[34:]

dd= np.asarray(train['GDP per capita (current US$)'])

y_hat_avg_gdp = test.copy()
fit_holt_linear = Holt(np.asarray(train['GDP per capita (current US$)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg_gdp['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['GDP per capita (current US$)'], label='train')
plt.plot(y_hat_avg_gdp['Holt_linear'], label='predictions')
plt.legend(loc='best')
plt.show()
y_hat_avg_gdp['Holt_linear']

y_hat_avg_gdp = y_hat_avg_gdp.iloc[1: , :]
y_hat_avg_gdp = y_hat_avg_gdp.drop(columns={'GDP per capita (current US$)'})
y_hat_avg_gdp = y_hat_avg_gdp.rename(columns={'Holt_linear': 'GDP per capita (current US$)'})
y_hat_avg_gdp['years_from_start'] = y_hat_avg_gdp['years_from_start'] + 1985
y_hat_avg_gdp = y_hat_avg_gdp.reset_index()
y_hat_avg_gdp = y_hat_avg_gdp.drop(columns={'Year'})
y_hat_avg_gdp = y_hat_avg_gdp.rename(columns={'years_from_start': 'Year'})
y_hat_avg_gdp

"""Individuals using the Internet (% of population) Future Predictions"""

India_df = full_df[full_df['Country'] == 'India']
India_df['Year'] = pd.to_datetime(India_df['Year'], format='%Y')
India_internet = India_df[['Year', 'Individuals using the Internet (% of population)']].reset_index()
India_internet = India_internet.drop(columns= {'index'})
India_internet = India_internet.set_index('Year')

lst = list(range(0,35))
arr = np.array(lst)
India_internet['years_from_start'] = arr
x = India_internet['years_from_start'].values.reshape(-1, 1)
y = India_internet['Individuals using the Internet (% of population)'].values
train=India_internet[0:28] 
test=India_internet[27:]
y_hat_avg = test.copy()
plt.figure(figsize=(16,8))
plt.plot(train['Individuals using the Internet (% of population)'], label='Train')
plt.plot(test['Individuals using the Internet (% of population)'], label='Test')
#Holt Linear 
y_hat_avg = test.copy()
fit_holt_linear = Holt(np.asarray(train['Individuals using the Internet (% of population)'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)
y_hat_avg['Holt_linear'] = fit_holt_linear.forecast(len(test))
plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')
plt.legend(loc='best')
rms_holt_linear = sqrt(mean_squared_error(test['Individuals using the Internet (% of population)'], y_hat_avg.Holt_linear))
#Holt Winter
y_hat_avg = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Individuals using the Internet (% of population)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')
plt.legend(loc='best')
rms_holt_winter = sqrt(mean_squared_error(test['Individuals using the Internet (% of population)'], y_hat_avg.Holt_Winter))
#ARIMA
y_hat_avg = test.copy()
fit_arima = sm.tsa.statespace.SARIMAX(train['Individuals using the Internet (% of population)'], order=(2, 1, 4)).fit()
y_hat_avg['SARIMA'] = fit_arima.predict(start="2011-01-01", end="2019-01-01", dynamic=True)
plt.plot(y_hat_avg['SARIMA'], label='SARIMA')
plt.legend(loc='best')
plt.show()
rms_arima = sqrt(mean_squared_error(test['Individuals using the Internet (% of population)'], y_hat_avg.SARIMA))
print("Holt Linear RMSE:", rms_holt_linear, "Holt Winter RMSE", rms_holt_winter, "ARIMA RMSE:", rms_arima)

India_df = full_df[full_df['Country'] == 'India']
India_internet = India_df[['Year', 'Individuals using the Internet (% of population)']].reset_index()
India_internet = India_internet.drop(columns= {'index'})
NaN = np.nan

India_internet.loc[len(India_internet.index)] = [2020, 0]
India_internet.loc[len(India_internet.index)] = [2021, 0]
India_internet.loc[len(India_internet.index)] = [2022, 0]
India_internet.loc[len(India_internet.index)] = [2023, 0]
India_internet.loc[len(India_internet.index)] = [2024, 0]
India_internet.loc[len(India_internet.index)] = [2025, 0]
India_internet.loc[len(India_internet.index)] = [2026, 0]
India_internet.loc[len(India_internet.index)] = [2027, 0]
India_internet.loc[len(India_internet.index)] = [2028, 0]
India_internet.loc[len(India_internet.index)] = [2029, 0]
India_internet.loc[len(India_internet.index)] = [2030, 0]
India_internet['Year'] = pd.to_datetime(India_internet['Year'], format='%Y')
India_internet = India_internet.set_index('Year')
lst = list(range(0,46))
arr = np.array(lst)
India_internet['years_from_start'] = arr
x = India_internet['years_from_start'].values.reshape(-1, 1)
y = India_internet['Individuals using the Internet (% of population)'].values
train=India_internet[0:35] 
test=India_internet[34:]

dd= np.asarray(train['Individuals using the Internet (% of population)'])

y_hat_avg_internet = test.copy()
fit_holt_winter = ExponentialSmoothing(np.asarray(train['Individuals using the Internet (% of population)']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()
y_hat_avg_internet['Holt_Winter'] = fit_holt_winter.forecast(len(test))
plt.figure(figsize=(16,8))
plt.plot(train['Individuals using the Internet (% of population)'], label='train')
plt.plot(y_hat_avg_internet['Holt_Winter'], label='predictions')
plt.legend(loc='best')
plt.show()

y_hat_avg_internet = y_hat_avg_internet.iloc[1: , :]
y_hat_avg_internet = y_hat_avg_internet.drop(columns={'Individuals using the Internet (% of population)'})
y_hat_avg_internet = y_hat_avg_internet.rename(columns={'Holt_Winter': 'Individuals using the Internet (% of population)'})
y_hat_avg_internet['years_from_start'] = y_hat_avg_internet['years_from_start'] + 1985
y_hat_avg_internet = y_hat_avg_internet.reset_index()
y_hat_avg_internet = y_hat_avg_internet.drop(columns={'Year'})
y_hat_avg_internet = y_hat_avg_internet.rename(columns={'years_from_start': 'Year'})
y_hat_avg_internet

predictions_df = y_hat_avg_c02
predictions_df['Renewable energy consumption (% of total final energy consumption)'] = y_hat_avg_energy['Renewable energy consumption (% of total final energy consumption)']
predictions_df['rural'] = y_hat_avg_rural['rural']
predictions_df['Birth rate, crude (per 1,000 people)'] = y_hat_avg_birth['Birth rate, crude (per 1,000 people)']
predictions_df['immunization'] = y_hat_avg_immunization['immunization']
predictions_df['Immunization, measles (% of children ages 12-23 months)'] = y_hat_avg_measles['Immunization, measles (% of children ages 12-23 months)']
predictions_df['Life expectancy at birth, total (years)'] = y_hat_avg_life['Life expectancy at birth, total (years)']
predictions_df['Mortality rate, under-5 (per 1,000 live births)'] = y_hat_avg_mortality['Mortality rate, under-5 (per 1,000 live births)']
predictions_df['Population growth (annual %)'] = y_hat_avg_population['Population growth (annual %)']
predictions_df['Adjusted savings: education expenditure (% of GNI)'] = y_hat_avg_education['Adjusted savings: education expenditure (% of GNI)']
predictions_df['GDP per capita (current US$)'] = y_hat_avg_gdp['GDP per capita (current US$)']
predictions_df['Individuals using the Internet (% of population)'] = y_hat_avg_internet['Individuals using the Internet (% of population)']
predictions_df

dataset = predictions_df.values
X = dataset[:,1:13]
min_max_scaler = preprocessing.MinMaxScaler()
predictions_scale = min_max_scaler.fit_transform(X)
final_predictions = model.predict(predictions_scale)

"""Now we go and use our previous neural net to predict values of development for India from 2020-2030 using the 12 indicator variables we calcualted using the methods explained above. 

"""

final_predictions

final_predictions_df = pd.DataFrame(final_predictions, columns = {'Prediction'})
final_predictions_df['Prediction'] = final_predictions_df['Prediction'].apply(lambda x : 1 if x > 0.5 else 0)
final_predictions_df['Year'] = predictions_df['Year']
final_predictions_df = final_predictions_df.set_index('Year')
final_predictions_df

"""As the 0s turn into 1s through our neural net predictions after 2026, we predict that India will become a developed country by 2026 based on our 12 indicators!"""